{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "description: class and functions to assist in the task or harmonising variables across\n",
    "  datasets\n",
    "output-file: data.html\n",
    "title: Data Harmonising\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.test import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from fastcore.utils import *\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "import pyreadstat\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for reading an SPSS file, converting the metadata into a dataframe, and then saving the data and metadata to parquet files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def reformat_metadata(meta: pyreadstat.metadata_container\n",
    "                      ) -> DataFrame:\n",
    "      \"Convert metadata from pyreadstat format into a pandas DataFrame.\"\n",
    "      # Not including 'align' or 'role', as pyreadstat does not include them\n",
    "      meta={\"Label\": meta.column_names_to_labels,\n",
    "            \"Values\": meta.variable_value_labels,\n",
    "            \"Type\": meta.original_variable_types,\n",
    "            \"Width\": meta.variable_display_width, \n",
    "            \"Measure\": meta.variable_measure}\n",
    "      # Convert metadata to DataFrame and transpose\n",
    "      return DataFrame(data={k: meta[k] for k in meta.keys()}).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "file = \"../data/G227_Q.sav\"\n",
    "_, meta = pyreadstat.read_sav(file)\n",
    "meta = reformat_metadata(meta)\n",
    "\n",
    "test_eq(type(meta), DataFrame)\n",
    "test_eq(meta.index, ['Label', 'Values', 'Type', 'Width', 'Measure'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def read_sav(file: str, # Path to SPSS file\n",
    "            ) -> Tuple[DataFrame, DataFrame]: # Output df and meta as dataframes\n",
    "      \"Wrapper around `pyreadstat.read_sav()` with nicer metadata output.\"\n",
    "      df, meta = pyreadstat.read_sav(file)\n",
    "      meta = reformat_metadata(meta)\n",
    "      return df, meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df, meta = read_sav(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sav_to_parquet(df: DataFrame, #\n",
    "                   meta: DataFrame, # \n",
    "                   filename: str, # Basename for saving files (ie. for G208_Q.sav, filename=\"G208_Q\")\n",
    "                   dir: str # Directory to save output\n",
    "                   ) -> None:\n",
    "      \"Save data and metadata as parquet files.\"\n",
    "      # Convert metadata to all string types so it behaves nicely when saving as a parquet file\n",
    "      meta = meta.astype(str)\n",
    "      df.to_parquet(Path(dir) / f\"{filename}_df.parquet\")\n",
    "      meta.to_parquet(Path(dir) / f\"{filename}_meta.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "df, meta = read_sav(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that there is no loss or corruption of data in the conversion process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pq = pd.read_parquet(\"../data/G227_Q_df.parquet\")\n",
    "test_eq(df, df_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta_pq = pd.read_parquet(\"../data/G227_Q_meta.parquet\")\n",
    "test_eq(meta.astype(str), meta_pq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Dataset:\n",
    "    \"A class which contains both the data and metadata for a given data file.\"\n",
    "    def __init__(self,\n",
    "                 df: DataFrame, # the actual raw data\n",
    "                 meta: DataFrame): # the metadata, including variable labels, value labels, and types for each variable\n",
    "        self.df, self.meta = df, meta"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
